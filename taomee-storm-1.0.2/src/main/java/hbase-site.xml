<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
/**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
-->
<configuration>
	<!--hBase集群中所有RegionServer共享目录，用来持久化Hbase的数据，一般设置的是hdfs的文件目录-->
	<property>  
		<name>hbase.rootdir</name>  
		<value>hdfs://10.1.1.228:9000/hbase</value>
	</property>  
	
	<property>
		<name>hbase.master.port</name>
		<value>60000</value>
	</property>

        <!--集群的模式，分布式还是单机模式-->
	<property>  
		<name>hbase.cluster.distributed</name>  
		<value>true</value>  
	</property>  
	
	<!--本地文件系统tmp目录-->
	<!--<property>
		<name>hbase.tmp.dir</name>  
		<value>/home/hadoop/tmp/hbase/hbase_tmp</value>  
	</property>  -->
	
	<!--zookeeper集群的URL配置-->
	<property>  
		<name>hbase.zookeeper.quorum</name>  
		<value>10.1.1.228,10.1.1.225,10.1.1.223</value>  
	</property>  
	
	<property>
		 <name>hbase.zookeeper.property.clientPort</name>
		 <value>2181</value>
	</property>
	
	<property>
		<name>hbase.master.info.port</name>
		<value>60010</value>
	</property>	

	<!--zookeeper的zoo.conf中的配置，快照的存储位置-->
	<!--<property>
		<name>hbase.zookeeper.property.dataDir</name>  
		<value>/home/hadoop/tmp/hbase/hbase_zookeeper</value>  
	</property>  -->
	
	<property>
		<name>hbase.hregion.max.filesize</name>
		<value>20971520</value>
	</property>

	 <property>
                 <name>hbase.hregion.memstore.flush.size</name>
	         <value>1048576</value>
	</property>
	<!--指定hdfs的nameservice为ns，需要和core-site.xml中的保持一致 -->
	<property>
		<name>dfs.nameservices</name>
		<value>ns</value>
	</property>

	<!-- ns下面有两个NameNode，分别是nn1，nn2 -->
	<property>
		<name>dfs.ha.namenodes.ns</name>
		<value>nn1,nn2</value>
	</property>

	<!-- nn1的RPC通信地址 -->
	<property>
		<name>dfs.namenode.rpc-address.ns.nn1</name>
		<value>10.1.1.228:9000</value>
	</property>

	<!-- nn1的http通信地址 -->
	<property>
		<name>dfs.namenode.http-address.ns.nn1</name>
		<value>10.1.1.228:50070</value>
	</property>

	<!-- nn2的RPC通信地址 -->
	<property>
		<name>dfs.namenode.rpc-address.ns.nn2</name>
		<value>10.1.1.225:9000</value>
	</property>

	<!-- nn2的http通信地址 -->
	<property>
		<name>dfs.namenode.http-address.ns.nn2</name>
		<value>10.1.1.225:50070</value>
	</property>

	<!-- 指定NameNode的元数据在JournalNode上的存放位置 -->
	<property>
		<name>dfs.namenode.shared.edits.dir</name>
		<value>qjournal://10.1.1.228:8485;10.1.1.225:8485;10.1.1.223:8485/ns</value>
	</property>

	<!-- 指定JournalNode在本地磁盘存放数据的位置 -->
	<!--<property>
		<name>dfs.journalnode.edits.dir</name>
		<value>/home/hadoop/data/hadoop/journal</value>
	</property>-->

	<!-- 开启NameNode故障时自动切换 -->
	<!--<property>
		<name>dfs.ha.automatic-failover.enabled</name>
		<value>true</value>
	</property>-->

	<!-- 配置失败自动切换实现方式 -->
	<!--<property>
		<name>dfs.client.failover.proxy.provider.ns</name>
		<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
	</property>-->

	<!-- 配置隔离机制 -->
	<!--<property>
		<name>dfs.ha.fencing.methods</name>
		<value>sshfence</value>
	</property>-->

	<!-- 使用隔离机制时需要ssh免登陆 -->
	<!--<property>
		<name>dfs.ha.fencing.ssh.private-key-files</name>
		<value>/home/hadoop/.ssh/id_rsa</value>
	</property>-->
	<!-- 如果格式化进程使用的话，会清空里面的所有数据，并且建立一个新的文件系统-->
	<!--<property>
		<name>dfs.namenode.name.dir</name>
		<value>file:///home/hadoop/data/hadoop/hdfs/name</value>
	</property>

	<property>
		<name>dfs.datanode.data.dir</name>
		<value>file:///home/hadoop/data/hadoop/hdfs/data</value>
	</property>

	<property>
		<name>dfs.replication</name>
		<value>2</value>
	</property>-->

	<!-- 在NN和DN上开启WebHDFS (REST API)功能,不是必须 -->
	<!--<property>
		<name>dfs.webhdfs.enabled</name>
		<value>true</value>
	</property>-->

	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://ns</value>
	</property>

	<property>
		<name>dfs.namenode.datanode.registration.ip-hostname-check</name>
		<value>false</value>
	</property>

	<property>
		<name>fs.hdfs.impl</name>
		<value>org.apache.hadoop.hdfs.DistributedFileSystem</value>
	</property>

</configuration>
